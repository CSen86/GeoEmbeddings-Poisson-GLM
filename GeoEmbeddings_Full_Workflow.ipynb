{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n# GeoEmbeddings for Poisson GLM \u2013 Full Educational Workflow\n\n## Conceptual Introduction\n\nTraditional actuarial Poisson GLMs use handcrafted features and exposure offsets to model claim frequency.  \nHowever, spatial risk is often complex, nonlinear, and multi-dimensional.\n\nInstead of feeding raw coordinates directly into a GLM, we first learn a **vector representation (embedding)** of geography using an autoencoder.\n\n### Why is this novel?\n\n- Raw geographic variables do not encode spatial similarity.\n- High-dimensional spatial features (hazard, census, terrain) interact nonlinearly.\n- Representation learning compresses spatial structure into dense vectors.\n- GLM remains interpretable while benefiting from richer spatial structure.\n\nThis idea is inspired by:\nBlier-Wong et al. (2021) \u2013 Geographic Ratemaking with Spatial Embeddings.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Imports"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nimport statsmodels.formula.api as smf\nfrom sklearn.metrics import mean_squared_error\n\nnp.random.seed(42)\ntorch.manual_seed(42)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Simulate Spatial and Traditional Data"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nN = 60000\n\nlongitude = np.random.uniform(-100,100,N)\nlatitude  = np.random.uniform(-100,100,N)\n\npop_density = np.exp(-(longitude**2+latitude**2)/20000) * np.random.uniform(10,300,N)\nmedian_income = 35000 + 10000*np.sin(longitude/20) + 8000*np.cos(latitude/25)\n\nflood_hazard = np.clip(np.sin(longitude/15)+np.cos(latitude/15),0,3)\nstorm_exposure = flood_hazard * np.random.rand(N)\n\nelevation = 0.005*(longitude**2+latitude**2)+5*np.random.randn(N)\nterrain_roughness = np.abs(np.gradient(elevation))\n\ninsured_value = np.random.uniform(50000,400000,N)\nbuilding_age  = np.random.randint(0,80,N)\npolicy_count  = np.random.randint(1,4,N)\nexposure      = np.random.randint(1,5,N)\n\nlinear_traditional = 0.000002*insured_value + 0.05*building_age + 0.3*policy_count\nlinear_spatial = 0.3*pop_density + 0.5*flood_hazard + 0.2*storm_exposure + 0.1*elevation\n\ntrue_linear = linear_traditional + linear_spatial\nrate = np.exp(true_linear)\nclaim_count = np.random.poisson(rate*exposure)\n\ndf = pd.DataFrame({\n    \"longitude\":longitude,\"latitude\":latitude,\n    \"pop_density\":pop_density,\"median_income\":median_income,\n    \"flood_hazard\":flood_hazard,\"storm_exposure\":storm_exposure,\n    \"elevation\":elevation,\"terrain_roughness\":terrain_roughness,\n    \"insured_value\":insured_value,\"building_age\":building_age,\n    \"policy_count\":policy_count,\"exposure\":exposure,\n    \"claim_count\":claim_count\n})\n\ndf.head()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Train/Test Split"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\ntrain = df.sample(frac=0.7, random_state=42)\ntest  = df.drop(train.index)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Train Spatial Autoencoder"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\ngeo_vars = [\"longitude\",\"latitude\",\"pop_density\",\"median_income\",\n            \"flood_hazard\",\"storm_exposure\",\"elevation\",\"terrain_roughness\"]\n\ngeo_train = torch.FloatTensor(train[geo_vars].values)\nloader = DataLoader(TensorDataset(geo_train), batch_size=1024, shuffle=True)\n\nclass GeoAutoencoder(nn.Module):\n    def __init__(self,input_dim,latent_dim=5):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim,64),\n            nn.ReLU(),\n            nn.Linear(64,latent_dim)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim,64),\n            nn.ReLU(),\n            nn.Linear(64,input_dim)\n        )\n    def forward(self,x):\n        z = self.encoder(x)\n        recon = self.decoder(z)\n        return z,recon\n\nmodel = GeoAutoencoder(len(geo_vars),5)\noptimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\ncriterion = nn.MSELoss()\n\nfor epoch in range(20):\n    total_loss = 0\n    for (xb,) in loader:\n        z,recon = model(xb)\n        loss = criterion(recon,xb)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss={total_loss/len(loader):.4f}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Extract Embeddings"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nwith torch.no_grad():\n    train_emb = model.encoder(torch.FloatTensor(train[geo_vars].values)).numpy()\n    test_emb  = model.encoder(torch.FloatTensor(test[geo_vars].values)).numpy()\n\nfor i in range(train_emb.shape[1]):\n    train[f\"emb_{i}\"] = train_emb[:,i]\n    test[f\"emb_{i}\"]  = test_emb[:,i]\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Baseline Poisson GLM"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nformula_base = \"claim_count ~ insured_value + building_age + policy_count + offset(np.log(exposure))\"\n\nbaseline_glm = smf.glm(\n    formula=formula_base,\n    data=train,\n    family=smf.families.Poisson()\n).fit()\n\nprint(baseline_glm.summary())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Enhanced Poisson GLM with Embeddings"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nemb_cols = [f\"emb_{i}\" for i in range(5)]\nformula_enh = \"claim_count ~ insured_value + building_age + policy_count + \"               + \" + \".join(emb_cols) + \" + offset(np.log(exposure))\"\n\nenhanced_glm = smf.glm(\n    formula=formula_enh,\n    data=train,\n    family=smf.families.Poisson()\n).fit()\n\nprint(enhanced_glm.summary())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. Evaluation"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\npred_base = baseline_glm.predict(test)\npred_enh  = enhanced_glm.predict(test)\n\nprint(\"Baseline MSE:\", mean_squared_error(test[\"claim_count\"],pred_base))\nprint(\"Enhanced MSE:\", mean_squared_error(test[\"claim_count\"],pred_enh))\n\nplt.scatter(test[\"claim_count\"],pred_base,alpha=0.3,label=\"Baseline\")\nplt.scatter(test[\"claim_count\"],pred_enh,alpha=0.3,label=\"Enhanced\")\nplt.legend()\nplt.title(\"Calibration Plot\")\nplt.show()\n"
    }
  ]
}